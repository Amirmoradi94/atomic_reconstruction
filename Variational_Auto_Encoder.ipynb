{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Auto_encoder.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "M0Rqs5y_wB-W"
      },
      "source": [
        "!pip install -U -q PyDrive\r\n",
        "\r\n",
        "from pydrive.auth import GoogleAuth\r\n",
        "from pydrive.drive import GoogleDrive\r\n",
        "from google.colab import auth\r\n",
        "from oauth2client.client import GoogleCredentials\r\n",
        "\r\n",
        "auth.authenticate_user()\r\n",
        "gauth = GoogleAuth()\r\n",
        "gauth.credentials = GoogleCredentials.get_application_default()\r\n",
        "drive = GoogleDrive(gauth)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VVzP4_jlwJiO"
      },
      "source": [
        "######## Import Libraries ########\r\n",
        "import numpy as np\r\n",
        "from google.colab import files\r\n",
        "import matplotlib.pyplot as plt \r\n",
        "import math, time, timeit, random\r\n",
        "\r\n",
        "import tensorflow\r\n",
        "from tensorflow.keras.optimizers import Adam\r\n",
        "from keras.models import model_from_json\r\n",
        "from keras import backend as K"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "63owd9Y1wJuF"
      },
      "source": [
        "# download Dataset (structures of atoms)\r\n",
        "id = [\"1mF_xVxamS4MZenCP8Id9H47ec6F-Ykdz\", \r\n",
        "      \"14oZchc1LwooFdfx5FNdhSkmpb8Kk-tC-\", \r\n",
        "      \"1IFdIDwzFsQlYS6syLT3_KVu8tddAMYCq\", \r\n",
        "      \"1ZXgvH5dkm7XxeqdMjBOP4csHqFFqepaw\",\r\n",
        "      \"1vmaP5B64_srduCChCNwxcH02Ks6JSNhc\",\r\n",
        "      \"1kVtcJPS71dZXBLLjzSriwpI2MCnxLlxn\",\r\n",
        "      \"1eRLHDsGU4FfQHko0OsX5fOgnxQrchvWP\",\r\n",
        "      \"11rhWFFevWa94y9tZGj0D45c5DS0xzhcy\",\r\n",
        "      \"1I2oFy-p8UfKJboVX9qTN876O3TKVoCDa\"]\r\n",
        "name = ['X_train', 'xyz_train', 'y_train', 'X_test', 'xyz_test', 'y_test', 'X_validation', 'xyz_val', 'y_validation']\r\n",
        "for i in range(len(id)):\r\n",
        "    download = drive.CreateFile({'id': id[i]})\r\n",
        "    download.GetContentFile('{}.npy'.format(name[i]))\r\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LdsyuL-HTgte"
      },
      "source": [
        "# Load the model of Decoder and VAE for testing purposes\r\n",
        "# Structure of the model has been saved as a .json file and weights have been saved as .h5\r\n",
        "download = drive.CreateFile({'id':\"1iwTXYOKpKQr4wk9aXATiRINNeDqJwTBt\"})\r\n",
        "download.GetContentFile('Decoder_Model.json')\r\n",
        "\r\n",
        "download = drive.CreateFile({'id':\"1ZcSgheWu1K3tmh88np21NoSthohzdAoU\"})\r\n",
        "download.GetContentFile('Decoder_Weights.h5')\r\n",
        "\r\n",
        "download = drive.CreateFile({'id':\"1gDmQNcj0iv1bQnnoMENijzLpVa7PMBpe\"})\r\n",
        "download.GetContentFile('VAE_Model0.json')\r\n",
        "\r\n",
        "download = drive.CreateFile({'id':\"1XbBcOhWfk4XMAnbVTSvfQmWgZKebY9QK\"})\r\n",
        "download.GetContentFile('VAE_Weights.h5')\r\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VSEQU9BmuWqi"
      },
      "source": [
        "## Load dataset from colab memory\r\n",
        "X_train = np.load(\"X_train.npy\")\r\n",
        "X_test = np.load(\"X_test.npy\")\r\n",
        "X_validation = np.load(\"X_validation.npy\")\r\n",
        "\r\n",
        "# Dataset Normalization (divide over maximum value of dataset)\r\n",
        "X_train /= 5.7\r\n",
        "X_test /= 5.7\r\n",
        "X_validation /= 5.7"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GrVTU2xQX6f9"
      },
      "source": [
        "# Load xyz dataset\r\n",
        "xyz_train = np.load(\"xyz_train.npy\")\r\n",
        "xyz_test = np.load(\"xyz_test.npy\")\r\n",
        "xyz_val = np.load(\"xyz_val.npy\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WchEji_tYARK"
      },
      "source": [
        "# Load Enrgies of each structure\r\n",
        "y_train = np.load(\"y_train.npy\")\r\n",
        "y_test = np.load(\"y_test.npy\")\r\n",
        "y_validation = np.load(\"y_validation.npy\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nDL2yXOFxS95",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cf7e3c15-f715-4b27-baae-0fcbe9b9311c"
      },
      "source": [
        "# Encoder\r\n",
        "input_data = tensorflow.keras.layers.Input(shape = (216, 5), name='input_encoder')\r\n",
        "\r\n",
        "encoder_conv1 = tensorflow.keras.layers.Conv1D(256, kernel_size = 3, padding='same')(input_data)\r\n",
        "encoder_act1 = tensorflow.keras.layers.LeakyReLU(alpha=0.1)(encoder_conv1)\r\n",
        "#encoder = tensorflow.keras.layers.BatchNormalization()(encoder)\r\n",
        "\r\n",
        "encoder_conv2 = tensorflow.keras.layers.Conv1D(128, kernel_size = 3, padding='same')(encoder_act1)\r\n",
        "encoder_act2 = tensorflow.keras.layers.LeakyReLU(alpha=0.1)(encoder_conv2)\r\n",
        "#encoder = tensorflow.keras.layers.BatchNormalization()(encoder)\r\n",
        "\r\n",
        "encoder_conv3 = tensorflow.keras.layers.Conv1D(64, kernel_size = 3, padding='same')(encoder_act2)\r\n",
        "encoder_act3 = tensorflow.keras.layers.LeakyReLU(alpha=0.1)(encoder_conv3)\r\n",
        "#encoder = tensorflow.keras.layers.BatchNormalization()(encoder)\r\n",
        "\r\n",
        "encoder_conv4 = tensorflow.keras.layers.Conv1D(32, kernel_size = 3, padding='same')(encoder_act3)\r\n",
        "encoder_act4 = tensorflow.keras.layers.LeakyReLU(alpha=0.1)(encoder_conv4)\r\n",
        "#encoder = tensorflow.keras.layers.BatchNormalization()(encoder)\r\n",
        "\r\n",
        "encoder_conv3 = tensorflow.keras.layers.Conv1D(1, kernel_size = 3, padding='same')(encoder_act4)\r\n",
        "encoder = tensorflow.keras.layers.LeakyReLU(alpha=0.1)(encoder_conv3)\r\n",
        "#encoder = tensorflow.keras.layers.BatchNormalization()(encoder)\r\n",
        "\r\n",
        "encoder_flatten = tensorflow.keras.layers.Flatten()(encoder)\r\n",
        "\r\n",
        "def sample_latent_features(distribution):\r\n",
        "    distribution_mean, distribution_variance = distribution\r\n",
        "    batch_size = tensorflow.shape(distribution_variance)[0]\r\n",
        "    random = tensorflow.keras.backend.random_normal(shape=(batch_size, tensorflow.shape(distribution_variance)[1]))\r\n",
        "    return distribution_mean + tensorflow.exp(0.5 * distribution_variance) * random\r\n",
        "\r\n",
        "distribution_mean = tensorflow.keras.layers.Dense(216, name='mean')(encoder_flatten)\r\n",
        "distribution_variance = tensorflow.keras.layers.Dense(216, name='variance')(encoder_flatten)\r\n",
        "latent_encoding = tensorflow.keras.layers.Lambda(sample_latent_features)([distribution_mean, distribution_variance])\r\n",
        "\r\n",
        "encoder_model = tensorflow.keras.Model(input_data, latent_encoding)\r\n",
        "encoder_model.summary()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"model_2\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_encoder (InputLayer)      [(None, 216, 5)]     0                                            \n",
            "__________________________________________________________________________________________________\n",
            "conv1d_10 (Conv1D)              (None, 216, 256)     4096        input_encoder[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "leaky_re_lu_10 (LeakyReLU)      (None, 216, 256)     0           conv1d_10[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "conv1d_11 (Conv1D)              (None, 216, 128)     98432       leaky_re_lu_10[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "leaky_re_lu_11 (LeakyReLU)      (None, 216, 128)     0           conv1d_11[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "conv1d_12 (Conv1D)              (None, 216, 64)      24640       leaky_re_lu_11[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "leaky_re_lu_12 (LeakyReLU)      (None, 216, 64)      0           conv1d_12[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "conv1d_13 (Conv1D)              (None, 216, 32)      6176        leaky_re_lu_12[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "leaky_re_lu_13 (LeakyReLU)      (None, 216, 32)      0           conv1d_13[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "conv1d_14 (Conv1D)              (None, 216, 1)       97          leaky_re_lu_13[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "leaky_re_lu_14 (LeakyReLU)      (None, 216, 1)       0           conv1d_14[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "flatten_2 (Flatten)             (None, 216)          0           leaky_re_lu_14[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "mean (Dense)                    (None, 216)          46872       flatten_2[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "variance (Dense)                (None, 216)          46872       flatten_2[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "lambda_2 (Lambda)               (None, 216)          0           mean[0][0]                       \n",
            "                                                                 variance[0][0]                   \n",
            "==================================================================================================\n",
            "Total params: 227,185\n",
            "Trainable params: 227,185\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FPASP6PGx9o1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "693857b2-6e93-400e-d673-f58944254235"
      },
      "source": [
        "decoder_input = tensorflow.keras.layers.Input(shape = (216), name='input_encoder')\r\n",
        "\r\n",
        "decoder_dense1 = tensorflow.keras.layers.Dense(216)(decoder_input)\r\n",
        "decoder_reshape = tensorflow.keras.layers.Reshape((216, 1))(decoder_dense1)\r\n",
        "\r\n",
        "decoder_conv1 = tensorflow.keras.layers.Conv1D(256, kernel_size = 3, padding='same')(decoder_reshape)\r\n",
        "decoder_act1 = tensorflow.keras.layers.LeakyReLU(alpha=0.1)(decoder_conv1)\r\n",
        "#decoder = tensorflow.keras.layers.BatchNormalization()(decoder_act1)\r\n",
        "\r\n",
        "decoder_conv2 = tensorflow.keras.layers.Conv1D(128, kernel_size = 3, padding='same')(decoder_act1)\r\n",
        "decoder_act2 = tensorflow.keras.layers.LeakyReLU(alpha=0.1)(decoder_conv2)\r\n",
        "#decoder = tensorflow.keras.layers.BatchNormalization()(decoder)\r\n",
        "\r\n",
        "decoder_conv3 = tensorflow.keras.layers.Conv1D(64, kernel_size = 3, padding='same')(decoder_act2)\r\n",
        "decoder_act3 = tensorflow.keras.layers.LeakyReLU(alpha=0.1)(decoder_conv3)\r\n",
        "#decoder = tensorflow.keras.layers.BatchNormalization()(decoder)\r\n",
        "\r\n",
        "decoder_conv4 = tensorflow.keras.layers.Conv1D(32, kernel_size = 3, padding='same')(decoder_act3)\r\n",
        "decoder_act4 = tensorflow.keras.layers.LeakyReLU(alpha=0.1)(decoder_conv4)\r\n",
        "#decoder = tensorflow.keras.layers.BatchNormalization()(decoder)\r\n",
        "\r\n",
        "\r\n",
        "decoder_output = tensorflow.keras.layers.Conv1D(3, kernel_size = 3, padding='same')(decoder_act4)\r\n",
        "\r\n",
        "decoder_model = tensorflow.keras.Model(decoder_input, decoder_output)\r\n",
        "decoder_model.summary()\r\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"model_3\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "input_encoder (InputLayer)   [(None, 216)]             0         \n",
            "_________________________________________________________________\n",
            "dense (Dense)                (None, 216)               46872     \n",
            "_________________________________________________________________\n",
            "reshape (Reshape)            (None, 216, 1)            0         \n",
            "_________________________________________________________________\n",
            "conv1d_15 (Conv1D)           (None, 216, 256)          1024      \n",
            "_________________________________________________________________\n",
            "leaky_re_lu_15 (LeakyReLU)   (None, 216, 256)          0         \n",
            "_________________________________________________________________\n",
            "conv1d_16 (Conv1D)           (None, 216, 128)          98432     \n",
            "_________________________________________________________________\n",
            "leaky_re_lu_16 (LeakyReLU)   (None, 216, 128)          0         \n",
            "_________________________________________________________________\n",
            "conv1d_17 (Conv1D)           (None, 216, 64)           24640     \n",
            "_________________________________________________________________\n",
            "leaky_re_lu_17 (LeakyReLU)   (None, 216, 64)           0         \n",
            "_________________________________________________________________\n",
            "conv1d_18 (Conv1D)           (None, 216, 32)           6176      \n",
            "_________________________________________________________________\n",
            "leaky_re_lu_18 (LeakyReLU)   (None, 216, 32)           0         \n",
            "_________________________________________________________________\n",
            "conv1d_19 (Conv1D)           (None, 216, 3)            291       \n",
            "=================================================================\n",
            "Total params: 177,435\n",
            "Trainable params: 177,435\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lDuaMh3D4u3x",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "614e98ca-11d5-49f8-d3fb-f398ec7c30b7"
      },
      "source": [
        "vae_input = tensorflow.keras.layers.Input(shape=(216,5), name=\"VAE_input\")\r\n",
        "encode = encoder_model(vae_input)\r\n",
        "decode = decoder_model(encode)\r\n",
        "vae = tensorflow.keras.Model(vae_input, decode)\r\n",
        "vae.summary()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"model_4\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "VAE_input (InputLayer)       [(None, 216, 5)]          0         \n",
            "_________________________________________________________________\n",
            "model_2 (Functional)         (None, 216)               227185    \n",
            "_________________________________________________________________\n",
            "model_3 (Functional)         (None, 216, 3)            177435    \n",
            "=================================================================\n",
            "Total params: 404,620\n",
            "Trainable params: 404,620\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ra0DOCo56YLf"
      },
      "source": [
        "def get_loss(distribution_mean, distribution_variance):\r\n",
        "    \r\n",
        "    def get_reconstruction_loss(y_pred, y_true):\r\n",
        "        reconstruction_loss = tensorflow.keras.losses.mse(y_true, y_pred)\r\n",
        "        reconstruction_loss_batch = tensorflow.reduce_mean(reconstruction_loss)\r\n",
        "        return reconstruction_loss_batch\r\n",
        "    \r\n",
        "    def get_kl_loss(distribution_mean, distribution_variance):\r\n",
        "        kl_loss = 1 + distribution_variance - tensorflow.square(distribution_mean) - tensorflow.exp(distribution_variance)\r\n",
        "        kl_loss_batch = tensorflow.reduce_mean(kl_loss)\r\n",
        "        return kl_loss_batch * (-0.5)\r\n",
        "    \r\n",
        "    def total_loss(y_true, y_pred):\r\n",
        "        reconstruction_loss_batch = get_reconstruction_loss(y_pred, y_true)\r\n",
        "        kl_loss_batch = get_kl_loss(distribution_mean, distribution_variance)\r\n",
        "        return kl_loss_batch + reconstruction_loss_batch\r\n",
        "    \r\n",
        "    return total_loss\r\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0fBlgmps82Ff"
      },
      "source": [
        "vae.compile(loss='mse', optimizer= 'rmsprop' ) #get_loss(distribution_mean, distribution_variance) # Adam(learning_rate=0.0005, beta_1=0.5, beta_2=0.999, epsilon=1e-07)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2ytCwJSRZNxk"
      },
      "source": [
        "dis_input = tensorflow.keras.layers.Input(shape = (256), name='input_discriminator')\r\n",
        "\r\n",
        "discriminator = tensorflow.keras.layers.Dense(512)(dis_input)\r\n",
        "discriminator = tensorflow.keras.layers.LeakyReLU(alpha=0.1)(discriminator)\r\n",
        "\r\n",
        "discriminator = tensorflow.keras.layers.Dense(256)(discriminator)\r\n",
        "discriminator = tensorflow.keras.layers.LeakyReLU(alpha=0.1)(discriminator)\r\n",
        "\r\n",
        "discriminator = tensorflow.keras.layers.Dense(128)(discriminator)\r\n",
        "discriminator = tensorflow.keras.layers.LeakyReLU(alpha=0.1)(discriminator)\r\n",
        "\r\n",
        "discriminator = tensorflow.keras.layers.Dense(64)(discriminator)\r\n",
        "discriminator = tensorflow.keras.layers.LeakyReLU(alpha=0.1)(discriminator)\r\n",
        "\r\n",
        "discriminator = tensorflow.keras.layers.Dense(32)(discriminator)\r\n",
        "discriminator = tensorflow.keras.layers.LeakyReLU(alpha=0.1)(discriminator)\r\n",
        "\r\n",
        "dis_output = tensorflow.keras.layers.Dense(1, activation='sigmoid')(discriminator)\r\n",
        "\r\n",
        "discriminator_model = tensorflow.keras.Model(dis_input, dis_output)\r\n",
        "discriminator_model.summary()\r\n",
        "\r\n",
        "discriminator_model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\r\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CjHFa85687Y6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "3dab8332-4ef0-43b4-f4b3-fbc0a724629d"
      },
      "source": [
        "history = vae.fit(X_train, xyz_train, epochs=400, batch_size=32, validation_data=(X_validation, xyz_val))\r\n",
        "\r\n",
        "vae_json = vae.to_json()\r\n",
        "decoder_json = decoder_model.to_json()\r\n",
        "\r\n",
        "with open(\"VAE_Model_try71.json\", \"w\") as json_file:\r\n",
        "    json_file.write(vae_json)\r\n",
        "with open(\"Decoder_Model_try71.json\", \"w\") as json_file:\r\n",
        "    json_file.write(decoder_json)\r\n",
        "\r\n",
        "vae.save_weights(\"VAE_Weights_try71.h5\")\r\n",
        "decoder_model.save_weights(\"Decoder_Weights_try71.h5\")\r\n",
        "\r\n",
        "files.download(\"VAE_Weights_try71.h5\")\r\n",
        "files.download(\"VAE_Model_try71.json\")\r\n",
        "\r\n",
        "files.download(\"Decoder_Model_try71.json\")\r\n",
        "files.download(\"Decoder_Weights_try71.h5\")\r\n",
        "\r\n",
        "# summarize history for loss\r\n",
        "fig = plt.figure()\r\n",
        "#plt.plot(history.history['mean_squared_error'])\r\n",
        "plt.plot(history.history['val_loss'])\r\n",
        "plt.title('Validation Mean Squared Error')\r\n",
        "plt.ylabel('MSE')\r\n",
        "plt.xlabel('epoch')\r\n",
        "plt.legend(['Validation'], loc='upper left')\r\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/400\n",
            "2188/2188 [==============================] - 28s 9ms/step - loss: 5.1225 - val_loss: 1.8324\n",
            "Epoch 2/400\n",
            "2188/2188 [==============================] - 19s 9ms/step - loss: 1.4160 - val_loss: 1.0641\n",
            "Epoch 3/400\n",
            "2188/2188 [==============================] - 19s 9ms/step - loss: 1.0122 - val_loss: 0.8500\n",
            "Epoch 4/400\n",
            "2188/2188 [==============================] - 19s 9ms/step - loss: 0.8632 - val_loss: 0.7498\n",
            "Epoch 5/400\n",
            "2188/2188 [==============================] - 19s 9ms/step - loss: 0.7580 - val_loss: 0.6709\n",
            "Epoch 6/400\n",
            "2188/2188 [==============================] - 19s 9ms/step - loss: 0.6800 - val_loss: 0.6376\n",
            "Epoch 7/400\n",
            "2188/2188 [==============================] - 19s 9ms/step - loss: 0.6292 - val_loss: 0.5972\n",
            "Epoch 8/400\n",
            "2188/2188 [==============================] - 20s 9ms/step - loss: 0.5778 - val_loss: 0.5724\n",
            "Epoch 9/400\n",
            "2188/2188 [==============================] - 20s 9ms/step - loss: 0.5505 - val_loss: 0.4918\n",
            "Epoch 10/400\n",
            "2188/2188 [==============================] - 20s 9ms/step - loss: 0.5254 - val_loss: 0.5410\n",
            "Epoch 11/400\n",
            "2188/2188 [==============================] - 20s 9ms/step - loss: 0.5066 - val_loss: 0.5039\n",
            "Epoch 12/400\n",
            "2188/2188 [==============================] - 20s 9ms/step - loss: 0.4896 - val_loss: 0.5196\n",
            "Epoch 13/400\n",
            "2188/2188 [==============================] - 20s 9ms/step - loss: 0.4684 - val_loss: 0.5304\n",
            "Epoch 14/400\n",
            "2188/2188 [==============================] - 20s 9ms/step - loss: 0.4479 - val_loss: 0.4839\n",
            "Epoch 15/400\n",
            "2188/2188 [==============================] - 19s 9ms/step - loss: 0.4431 - val_loss: 0.4200\n",
            "Epoch 16/400\n",
            "2188/2188 [==============================] - 20s 9ms/step - loss: 0.4277 - val_loss: 0.5102\n",
            "Epoch 17/400\n",
            "2188/2188 [==============================] - 20s 9ms/step - loss: 0.4182 - val_loss: 0.4271\n",
            "Epoch 18/400\n",
            "2188/2188 [==============================] - 19s 9ms/step - loss: 0.4107 - val_loss: 0.4758\n",
            "Epoch 19/400\n",
            "2188/2188 [==============================] - 20s 9ms/step - loss: 0.4076 - val_loss: 0.4283\n",
            "Epoch 20/400\n",
            "2188/2188 [==============================] - 20s 9ms/step - loss: 0.3945 - val_loss: 0.4335\n",
            "Epoch 21/400\n",
            "2188/2188 [==============================] - 20s 9ms/step - loss: 0.3835 - val_loss: 0.4031\n",
            "Epoch 22/400\n",
            "2188/2188 [==============================] - 20s 9ms/step - loss: 0.3844 - val_loss: 0.4356\n",
            "Epoch 23/400\n",
            "2188/2188 [==============================] - 20s 9ms/step - loss: 0.3763 - val_loss: 0.4483\n",
            "Epoch 24/400\n",
            "2188/2188 [==============================] - 20s 9ms/step - loss: 0.3685 - val_loss: 0.3840\n",
            "Epoch 25/400\n",
            "2188/2188 [==============================] - 20s 9ms/step - loss: 0.3741 - val_loss: 0.3853\n",
            "Epoch 26/400\n",
            "2188/2188 [==============================] - 20s 9ms/step - loss: 0.3642 - val_loss: 0.3892\n",
            "Epoch 27/400\n",
            "2188/2188 [==============================] - 20s 9ms/step - loss: 0.3625 - val_loss: 0.3745\n",
            "Epoch 28/400\n",
            "2188/2188 [==============================] - 20s 9ms/step - loss: 0.3592 - val_loss: 0.3645\n",
            "Epoch 29/400\n",
            "2188/2188 [==============================] - 20s 9ms/step - loss: 0.3539 - val_loss: 0.3734\n",
            "Epoch 30/400\n",
            "2188/2188 [==============================] - 20s 9ms/step - loss: 0.3477 - val_loss: 0.3661\n",
            "Epoch 31/400\n",
            "2188/2188 [==============================] - 20s 9ms/step - loss: 0.3452 - val_loss: 0.3400\n",
            "Epoch 32/400\n",
            "2188/2188 [==============================] - 20s 9ms/step - loss: 0.3471 - val_loss: 0.3429\n",
            "Epoch 33/400\n",
            "2188/2188 [==============================] - 20s 9ms/step - loss: 0.3404 - val_loss: 0.3315\n",
            "Epoch 34/400\n",
            "2188/2188 [==============================] - 20s 9ms/step - loss: 0.3337 - val_loss: 0.3706\n",
            "Epoch 35/400\n",
            "2188/2188 [==============================] - 20s 9ms/step - loss: 0.3358 - val_loss: 0.3318\n",
            "Epoch 36/400\n",
            "2188/2188 [==============================] - 20s 9ms/step - loss: 0.3360 - val_loss: 0.3350\n",
            "Epoch 37/400\n",
            "2188/2188 [==============================] - 20s 9ms/step - loss: 0.3307 - val_loss: 0.3322\n",
            "Epoch 38/400\n",
            "2188/2188 [==============================] - 20s 9ms/step - loss: 0.3242 - val_loss: 0.3599\n",
            "Epoch 39/400\n",
            "2188/2188 [==============================] - 20s 9ms/step - loss: 0.3266 - val_loss: 0.3287\n",
            "Epoch 40/400\n",
            "2188/2188 [==============================] - 20s 9ms/step - loss: 0.3243 - val_loss: 0.3377\n",
            "Epoch 41/400\n",
            "2188/2188 [==============================] - 20s 9ms/step - loss: 0.3172 - val_loss: 0.3566\n",
            "Epoch 42/400\n",
            "2188/2188 [==============================] - 20s 9ms/step - loss: 0.3151 - val_loss: 0.3953\n",
            "Epoch 43/400\n",
            "2188/2188 [==============================] - 20s 9ms/step - loss: 0.3196 - val_loss: 0.3649\n",
            "Epoch 44/400\n",
            "2188/2188 [==============================] - 20s 9ms/step - loss: 0.3195 - val_loss: 0.3105\n",
            "Epoch 45/400\n",
            "2188/2188 [==============================] - 20s 9ms/step - loss: 0.3090 - val_loss: 0.3133\n",
            "Epoch 46/400\n",
            "2188/2188 [==============================] - 19s 9ms/step - loss: 0.3100 - val_loss: 0.4360\n",
            "Epoch 47/400\n",
            "2188/2188 [==============================] - 20s 9ms/step - loss: 0.3131 - val_loss: 0.3526\n",
            "Epoch 48/400\n",
            "2188/2188 [==============================] - 20s 9ms/step - loss: 0.3108 - val_loss: 0.3615\n",
            "Epoch 49/400\n",
            "2188/2188 [==============================] - 20s 9ms/step - loss: 0.3048 - val_loss: 0.3508\n",
            "Epoch 50/400\n",
            "2188/2188 [==============================] - 20s 9ms/step - loss: 0.3023 - val_loss: 0.3349\n",
            "Epoch 51/400\n",
            "2188/2188 [==============================] - 20s 9ms/step - loss: 0.3033 - val_loss: 0.3144\n",
            "Epoch 52/400\n",
            "2188/2188 [==============================] - 19s 9ms/step - loss: 0.3037 - val_loss: 0.3519\n",
            "Epoch 53/400\n",
            "2188/2188 [==============================] - 20s 9ms/step - loss: 0.3004 - val_loss: 0.3441\n",
            "Epoch 54/400\n",
            "2188/2188 [==============================] - 20s 9ms/step - loss: 0.3010 - val_loss: 0.3842\n",
            "Epoch 55/400\n",
            "2188/2188 [==============================] - 20s 9ms/step - loss: 0.2972 - val_loss: 0.3318\n",
            "Epoch 56/400\n",
            "2188/2188 [==============================] - 20s 9ms/step - loss: 0.2968 - val_loss: 0.3576\n",
            "Epoch 57/400\n",
            "2188/2188 [==============================] - 20s 9ms/step - loss: 0.3002 - val_loss: 0.3952\n",
            "Epoch 58/400\n",
            "2188/2188 [==============================] - 20s 9ms/step - loss: 0.2941 - val_loss: 0.3192\n",
            "Epoch 59/400\n",
            "2188/2188 [==============================] - 20s 9ms/step - loss: 0.2982 - val_loss: 0.3033\n",
            "Epoch 60/400\n",
            "2188/2188 [==============================] - 20s 9ms/step - loss: 0.2923 - val_loss: 0.3668\n",
            "Epoch 61/400\n",
            "2188/2188 [==============================] - 20s 9ms/step - loss: 0.2918 - val_loss: 0.2917\n",
            "Epoch 62/400\n",
            "2188/2188 [==============================] - 20s 9ms/step - loss: 0.2889 - val_loss: 0.3300\n",
            "Epoch 63/400\n",
            "2188/2188 [==============================] - 20s 9ms/step - loss: 0.2911 - val_loss: 0.2899\n",
            "Epoch 64/400\n",
            "2188/2188 [==============================] - 20s 9ms/step - loss: 0.2890 - val_loss: 0.3171\n",
            "Epoch 65/400\n",
            "2188/2188 [==============================] - 20s 9ms/step - loss: 0.2866 - val_loss: 0.3741\n",
            "Epoch 66/400\n",
            "2188/2188 [==============================] - 20s 9ms/step - loss: 0.2906 - val_loss: 0.2986\n",
            "Epoch 67/400\n",
            "2188/2188 [==============================] - 20s 9ms/step - loss: 0.2900 - val_loss: 0.2931\n",
            "Epoch 68/400\n",
            "2188/2188 [==============================] - 20s 9ms/step - loss: 0.2860 - val_loss: 0.3397\n",
            "Epoch 69/400\n",
            "2188/2188 [==============================] - 20s 9ms/step - loss: 0.2876 - val_loss: 0.3090\n",
            "Epoch 70/400\n",
            "2188/2188 [==============================] - 20s 9ms/step - loss: 0.2826 - val_loss: 0.2816\n",
            "Epoch 71/400\n",
            "2188/2188 [==============================] - 20s 9ms/step - loss: 0.2861 - val_loss: 0.3564\n",
            "Epoch 72/400\n",
            "2188/2188 [==============================] - 20s 9ms/step - loss: 0.2836 - val_loss: 0.3166\n",
            "Epoch 73/400\n",
            "2188/2188 [==============================] - 20s 9ms/step - loss: 0.2848 - val_loss: 0.3189\n",
            "Epoch 74/400\n",
            "2188/2188 [==============================] - 20s 9ms/step - loss: 0.2790 - val_loss: 0.3304\n",
            "Epoch 75/400\n",
            "2188/2188 [==============================] - 20s 9ms/step - loss: 0.2780 - val_loss: 0.3701\n",
            "Epoch 76/400\n",
            "2188/2188 [==============================] - 20s 9ms/step - loss: 0.2785 - val_loss: 0.3277\n",
            "Epoch 77/400\n",
            "2188/2188 [==============================] - 20s 9ms/step - loss: 0.2806 - val_loss: 0.3320\n",
            "Epoch 78/400\n",
            "2188/2188 [==============================] - 20s 9ms/step - loss: 0.2792 - val_loss: 0.2984\n",
            "Epoch 79/400\n",
            "2188/2188 [==============================] - 20s 9ms/step - loss: 0.2754 - val_loss: 0.3347\n",
            "Epoch 80/400\n",
            "2188/2188 [==============================] - 20s 9ms/step - loss: 0.2745 - val_loss: 0.2800\n",
            "Epoch 81/400\n",
            "2188/2188 [==============================] - 20s 9ms/step - loss: 0.2746 - val_loss: 0.2753\n",
            "Epoch 82/400\n",
            "2188/2188 [==============================] - 20s 9ms/step - loss: 0.2752 - val_loss: 0.2778\n",
            "Epoch 83/400\n",
            "2188/2188 [==============================] - 20s 9ms/step - loss: 0.2762 - val_loss: 0.2730\n",
            "Epoch 84/400\n",
            "2188/2188 [==============================] - 20s 9ms/step - loss: 0.2745 - val_loss: 0.3421\n",
            "Epoch 85/400\n",
            "2188/2188 [==============================] - 20s 9ms/step - loss: 0.2734 - val_loss: 0.2894\n",
            "Epoch 86/400\n",
            "2188/2188 [==============================] - 20s 9ms/step - loss: 0.2766 - val_loss: 0.2807\n",
            "Epoch 87/400\n",
            "2188/2188 [==============================] - 20s 9ms/step - loss: 0.2718 - val_loss: 0.3257\n",
            "Epoch 88/400\n",
            "2188/2188 [==============================] - 20s 9ms/step - loss: 0.2719 - val_loss: 0.3136\n",
            "Epoch 89/400\n",
            "2188/2188 [==============================] - 20s 9ms/step - loss: 0.2710 - val_loss: 0.3835\n",
            "Epoch 90/400\n",
            "2188/2188 [==============================] - 20s 9ms/step - loss: 0.2684 - val_loss: 0.3160\n",
            "Epoch 91/400\n",
            "2188/2188 [==============================] - 20s 9ms/step - loss: 0.2703 - val_loss: 0.2968\n",
            "Epoch 92/400\n",
            "2188/2188 [==============================] - 20s 9ms/step - loss: 0.2732 - val_loss: 0.3398\n",
            "Epoch 93/400\n",
            "2188/2188 [==============================] - 20s 9ms/step - loss: 0.2693 - val_loss: 0.2795\n",
            "Epoch 94/400\n",
            "2188/2188 [==============================] - 20s 9ms/step - loss: 0.2651 - val_loss: 0.3377\n",
            "Epoch 95/400\n",
            "2188/2188 [==============================] - 20s 9ms/step - loss: 0.2677 - val_loss: 0.3302\n",
            "Epoch 96/400\n",
            "2188/2188 [==============================] - 20s 9ms/step - loss: 0.2723 - val_loss: 0.2952\n",
            "Epoch 97/400\n",
            "2188/2188 [==============================] - 20s 9ms/step - loss: 0.2670 - val_loss: 0.3190\n",
            "Epoch 98/400\n",
            "2188/2188 [==============================] - 20s 9ms/step - loss: 0.2681 - val_loss: 0.2879\n",
            "Epoch 99/400\n",
            "2188/2188 [==============================] - 20s 9ms/step - loss: 0.2683 - val_loss: 0.2914\n",
            "Epoch 100/400\n",
            "2188/2188 [==============================] - 20s 9ms/step - loss: 0.2736 - val_loss: 0.3013\n",
            "Epoch 101/400\n",
            "2188/2188 [==============================] - 20s 9ms/step - loss: 0.2725 - val_loss: 0.2707\n",
            "Epoch 102/400\n",
            "2188/2188 [==============================] - 20s 9ms/step - loss: 0.2630 - val_loss: 0.3330\n",
            "Epoch 103/400\n",
            "2188/2188 [==============================] - 20s 9ms/step - loss: 0.2618 - val_loss: 0.2930\n",
            "Epoch 104/400\n",
            "2188/2188 [==============================] - 20s 9ms/step - loss: 0.2666 - val_loss: 0.2799\n",
            "Epoch 105/400\n",
            "2188/2188 [==============================] - 20s 9ms/step - loss: 0.2606 - val_loss: 0.2955\n",
            "Epoch 106/400\n",
            "2188/2188 [==============================] - 20s 9ms/step - loss: 0.2612 - val_loss: 0.2966\n",
            "Epoch 107/400\n",
            "2188/2188 [==============================] - 20s 9ms/step - loss: 0.2609 - val_loss: 0.2967\n",
            "Epoch 108/400\n",
            "2188/2188 [==============================] - 20s 9ms/step - loss: 0.2634 - val_loss: 0.2723\n",
            "Epoch 109/400\n",
            "2188/2188 [==============================] - 20s 9ms/step - loss: 0.2637 - val_loss: 0.3075\n",
            "Epoch 110/400\n",
            "2188/2188 [==============================] - 20s 9ms/step - loss: 0.2604 - val_loss: 0.3149\n",
            "Epoch 111/400\n",
            "2188/2188 [==============================] - 20s 9ms/step - loss: 0.2594 - val_loss: 0.2817\n",
            "Epoch 112/400\n",
            "2188/2188 [==============================] - 20s 9ms/step - loss: 0.2614 - val_loss: 0.3002\n",
            "Epoch 113/400\n",
            "2188/2188 [==============================] - 20s 9ms/step - loss: 0.2607 - val_loss: 0.2644\n",
            "Epoch 114/400\n",
            "2188/2188 [==============================] - 20s 9ms/step - loss: 0.2632 - val_loss: 0.3028\n",
            "Epoch 115/400\n",
            "2188/2188 [==============================] - 20s 9ms/step - loss: 0.2626 - val_loss: 0.2804\n",
            "Epoch 116/400\n",
            "2188/2188 [==============================] - 20s 9ms/step - loss: 0.2605 - val_loss: 0.2692\n",
            "Epoch 117/400\n",
            "2188/2188 [==============================] - 20s 9ms/step - loss: 0.2566 - val_loss: 0.3246\n",
            "Epoch 118/400\n",
            "2188/2188 [==============================] - 20s 9ms/step - loss: 0.2587 - val_loss: 0.3189\n",
            "Epoch 119/400\n",
            "2188/2188 [==============================] - 20s 9ms/step - loss: 0.2595 - val_loss: 0.2916\n",
            "Epoch 120/400\n",
            "2188/2188 [==============================] - 20s 9ms/step - loss: 0.2575 - val_loss: 0.2722\n",
            "Epoch 121/400\n",
            "2188/2188 [==============================] - 20s 9ms/step - loss: 0.2603 - val_loss: 0.2912\n",
            "Epoch 122/400\n",
            "2188/2188 [==============================] - 20s 9ms/step - loss: 0.2604 - val_loss: 0.3648\n",
            "Epoch 123/400\n",
            "2188/2188 [==============================] - 20s 9ms/step - loss: 0.2572 - val_loss: 0.3368\n",
            "Epoch 124/400\n",
            "2188/2188 [==============================] - 20s 9ms/step - loss: 0.2576 - val_loss: 0.3151\n",
            "Epoch 125/400\n",
            "2188/2188 [==============================] - 20s 9ms/step - loss: 0.2558 - val_loss: 0.2852\n",
            "Epoch 126/400\n",
            "2188/2188 [==============================] - 20s 9ms/step - loss: 0.2567 - val_loss: 0.3409\n",
            "Epoch 127/400\n",
            "2188/2188 [==============================] - 20s 9ms/step - loss: 0.2546 - val_loss: 0.2841\n",
            "Epoch 128/400\n",
            "2188/2188 [==============================] - 20s 9ms/step - loss: 0.2558 - val_loss: 0.2868\n",
            "Epoch 129/400\n",
            "2188/2188 [==============================] - 20s 9ms/step - loss: 0.2588 - val_loss: 0.3443\n",
            "Epoch 130/400\n",
            "2188/2188 [==============================] - 20s 9ms/step - loss: 0.2572 - val_loss: 0.2978\n",
            "Epoch 131/400\n",
            "2188/2188 [==============================] - 20s 9ms/step - loss: 0.2558 - val_loss: 0.2701\n",
            "Epoch 132/400\n",
            "2188/2188 [==============================] - 20s 9ms/step - loss: 0.2536 - val_loss: 0.2910\n",
            "Epoch 133/400\n",
            "2188/2188 [==============================] - 20s 9ms/step - loss: 0.2553 - val_loss: 0.3140\n",
            "Epoch 134/400\n",
            "2188/2188 [==============================] - 20s 9ms/step - loss: 0.2522 - val_loss: 0.2589\n",
            "Epoch 135/400\n",
            "2188/2188 [==============================] - 20s 9ms/step - loss: 0.2548 - val_loss: 0.3621\n",
            "Epoch 136/400\n",
            "2188/2188 [==============================] - 20s 9ms/step - loss: 0.2538 - val_loss: 0.2766\n",
            "Epoch 137/400\n",
            "2188/2188 [==============================] - 20s 9ms/step - loss: 0.2507 - val_loss: 0.2802\n",
            "Epoch 138/400\n",
            "2188/2188 [==============================] - 20s 9ms/step - loss: 0.2554 - val_loss: 0.2585\n",
            "Epoch 139/400\n",
            "2188/2188 [==============================] - 20s 9ms/step - loss: 0.2539 - val_loss: 0.2967\n",
            "Epoch 140/400\n",
            "2188/2188 [==============================] - 20s 9ms/step - loss: 0.2511 - val_loss: 0.3066\n",
            "Epoch 141/400\n",
            "2188/2188 [==============================] - 20s 9ms/step - loss: 0.2493 - val_loss: 0.2638\n",
            "Epoch 142/400\n",
            "2188/2188 [==============================] - 20s 9ms/step - loss: 0.2516 - val_loss: 0.2520\n",
            "Epoch 143/400\n",
            "2188/2188 [==============================] - 20s 9ms/step - loss: 0.2480 - val_loss: 0.2918\n",
            "Epoch 144/400\n",
            "2188/2188 [==============================] - 20s 9ms/step - loss: 0.2537 - val_loss: 0.2849\n",
            "Epoch 145/400\n",
            "2188/2188 [==============================] - 20s 9ms/step - loss: 0.2516 - val_loss: 0.3385\n",
            "Epoch 146/400\n",
            "2188/2188 [==============================] - 20s 9ms/step - loss: 0.2506 - val_loss: 0.2965\n",
            "Epoch 147/400\n",
            "2188/2188 [==============================] - 20s 9ms/step - loss: 0.2486 - val_loss: 0.2645\n",
            "Epoch 148/400\n",
            "2188/2188 [==============================] - 20s 9ms/step - loss: 0.2507 - val_loss: 0.3298\n",
            "Epoch 149/400\n",
            "2188/2188 [==============================] - 20s 9ms/step - loss: 0.2513 - val_loss: 0.2576\n",
            "Epoch 150/400\n",
            "2188/2188 [==============================] - 20s 9ms/step - loss: 0.2497 - val_loss: 0.3208\n",
            "Epoch 151/400\n",
            "2188/2188 [==============================] - 20s 9ms/step - loss: 0.2479 - val_loss: 0.3249\n",
            "Epoch 152/400\n",
            "2188/2188 [==============================] - 20s 9ms/step - loss: 0.2509 - val_loss: 0.3162\n",
            "Epoch 153/400\n",
            "2188/2188 [==============================] - 20s 9ms/step - loss: 0.2534 - val_loss: 0.3696\n",
            "Epoch 154/400\n",
            "2188/2188 [==============================] - 20s 9ms/step - loss: 0.2533 - val_loss: 0.3280\n",
            "Epoch 155/400\n",
            "2188/2188 [==============================] - 20s 9ms/step - loss: 0.2466 - val_loss: 0.2730\n",
            "Epoch 156/400\n",
            "2188/2188 [==============================] - 20s 9ms/step - loss: 0.2472 - val_loss: 0.3342\n",
            "Epoch 157/400\n",
            "2188/2188 [==============================] - 20s 9ms/step - loss: 0.2507 - val_loss: 0.2843\n",
            "Epoch 158/400\n",
            "2188/2188 [==============================] - 20s 9ms/step - loss: 0.2489 - val_loss: 0.2625\n",
            "Epoch 159/400\n",
            "2188/2188 [==============================] - 20s 9ms/step - loss: 0.2452 - val_loss: 0.2697\n",
            "Epoch 160/400\n",
            "2188/2188 [==============================] - 20s 9ms/step - loss: 0.2494 - val_loss: 0.2935\n",
            "Epoch 161/400\n",
            "2188/2188 [==============================] - 20s 9ms/step - loss: 0.2474 - val_loss: 0.2768\n",
            "Epoch 162/400\n",
            "2188/2188 [==============================] - 20s 9ms/step - loss: 0.2458 - val_loss: 0.2460\n",
            "Epoch 163/400\n",
            "2188/2188 [==============================] - 20s 9ms/step - loss: 0.2463 - val_loss: 0.2559\n",
            "Epoch 164/400\n",
            "2188/2188 [==============================] - 20s 9ms/step - loss: 0.2481 - val_loss: 0.3133\n",
            "Epoch 165/400\n",
            "2188/2188 [==============================] - 20s 9ms/step - loss: 0.2465 - val_loss: 0.2814\n",
            "Epoch 166/400\n",
            "2188/2188 [==============================] - 20s 9ms/step - loss: 0.2483 - val_loss: 0.2623\n",
            "Epoch 167/400\n",
            "2188/2188 [==============================] - 20s 9ms/step - loss: 0.2488 - val_loss: 0.2796\n",
            "Epoch 168/400\n",
            "2188/2188 [==============================] - 20s 9ms/step - loss: 0.2496 - val_loss: 0.3042\n",
            "Epoch 169/400\n",
            "2188/2188 [==============================] - 20s 9ms/step - loss: 0.2485 - val_loss: 0.2557\n",
            "Epoch 170/400\n",
            "2188/2188 [==============================] - 20s 9ms/step - loss: 0.2502 - val_loss: 0.3174\n",
            "Epoch 171/400\n",
            "2188/2188 [==============================] - 20s 9ms/step - loss: 0.2429 - val_loss: 0.2932\n",
            "Epoch 172/400\n",
            "2188/2188 [==============================] - 20s 9ms/step - loss: 0.2446 - val_loss: 0.2583\n",
            "Epoch 173/400\n",
            "2188/2188 [==============================] - 20s 9ms/step - loss: 0.2437 - val_loss: 0.2607\n",
            "Epoch 174/400\n",
            "2188/2188 [==============================] - 20s 9ms/step - loss: 0.2491 - val_loss: 0.3057\n",
            "Epoch 175/400\n",
            "2188/2188 [==============================] - 20s 9ms/step - loss: 0.2442 - val_loss: 0.2892\n",
            "Epoch 176/400\n",
            "2188/2188 [==============================] - 20s 9ms/step - loss: 0.2452 - val_loss: 0.2797\n",
            "Epoch 177/400\n",
            "2188/2188 [==============================] - 20s 9ms/step - loss: 0.2464 - val_loss: 0.2755\n",
            "Epoch 178/400\n",
            "2188/2188 [==============================] - 20s 9ms/step - loss: 0.2455 - val_loss: 0.2528\n",
            "Epoch 179/400\n",
            "2188/2188 [==============================] - 20s 9ms/step - loss: 0.2469 - val_loss: 0.2708\n",
            "Epoch 180/400\n",
            "2188/2188 [==============================] - 20s 9ms/step - loss: 0.2423 - val_loss: 0.2662\n",
            "Epoch 181/400\n",
            "2188/2188 [==============================] - 20s 9ms/step - loss: 0.2432 - val_loss: 0.2591\n",
            "Epoch 182/400\n",
            "2188/2188 [==============================] - 20s 9ms/step - loss: 0.2445 - val_loss: 0.2527\n",
            "Epoch 183/400\n",
            "2188/2188 [==============================] - 20s 9ms/step - loss: 0.2463 - val_loss: 0.2911\n",
            "Epoch 184/400\n",
            "2188/2188 [==============================] - 20s 9ms/step - loss: 0.2448 - val_loss: 0.2768\n",
            "Epoch 185/400\n",
            "2188/2188 [==============================] - 20s 9ms/step - loss: 0.2422 - val_loss: 0.2627\n",
            "Epoch 186/400\n",
            "2188/2188 [==============================] - 20s 9ms/step - loss: 0.2401 - val_loss: 0.3170\n",
            "Epoch 187/400\n",
            "2188/2188 [==============================] - 20s 9ms/step - loss: 0.2450 - val_loss: 0.2613\n",
            "Epoch 188/400\n",
            "2188/2188 [==============================] - 20s 9ms/step - loss: 0.2448 - val_loss: 0.3108\n",
            "Epoch 189/400\n",
            "2188/2188 [==============================] - 20s 9ms/step - loss: 0.2427 - val_loss: 0.2854\n",
            "Epoch 190/400\n",
            "2188/2188 [==============================] - 20s 9ms/step - loss: 0.2431 - val_loss: 0.2482\n",
            "Epoch 191/400\n",
            "2188/2188 [==============================] - 20s 9ms/step - loss: 0.2380 - val_loss: 0.2560\n",
            "Epoch 192/400\n",
            "2188/2188 [==============================] - 20s 9ms/step - loss: 0.2424 - val_loss: 0.2403\n",
            "Epoch 193/400\n",
            "2188/2188 [==============================] - 20s 9ms/step - loss: 0.2408 - val_loss: 0.2771\n",
            "Epoch 194/400\n",
            "2188/2188 [==============================] - 20s 9ms/step - loss: 0.2462 - val_loss: 0.2587\n",
            "Epoch 195/400\n",
            "2188/2188 [==============================] - 20s 9ms/step - loss: 0.2391 - val_loss: 0.2795\n",
            "Epoch 196/400\n",
            "2188/2188 [==============================] - 20s 9ms/step - loss: 0.2395 - val_loss: 0.3161\n",
            "Epoch 197/400\n",
            "2188/2188 [==============================] - 20s 9ms/step - loss: 0.2425 - val_loss: 0.2925\n",
            "Epoch 198/400\n",
            "2188/2188 [==============================] - 20s 9ms/step - loss: 0.2399 - val_loss: 0.3468\n",
            "Epoch 199/400\n",
            "2188/2188 [==============================] - 20s 9ms/step - loss: 0.2416 - val_loss: 0.2910\n",
            "Epoch 200/400\n",
            "2188/2188 [==============================] - 20s 9ms/step - loss: 0.2417 - val_loss: 0.2564\n",
            "Epoch 201/400\n",
            "2188/2188 [==============================] - 20s 9ms/step - loss: 0.2437 - val_loss: 0.2814\n",
            "Epoch 202/400\n",
            "2188/2188 [==============================] - 20s 9ms/step - loss: 0.2430 - val_loss: 0.2858\n",
            "Epoch 203/400\n",
            "2188/2188 [==============================] - 20s 9ms/step - loss: 0.2393 - val_loss: 0.2672\n",
            "Epoch 204/400\n",
            "2188/2188 [==============================] - 20s 9ms/step - loss: 0.2370 - val_loss: 0.2862\n",
            "Epoch 205/400\n",
            "2188/2188 [==============================] - 20s 9ms/step - loss: 0.2419 - val_loss: 0.2828\n",
            "Epoch 206/400\n",
            "2188/2188 [==============================] - 20s 9ms/step - loss: 0.2396 - val_loss: 0.2621\n",
            "Epoch 207/400\n",
            "2188/2188 [==============================] - 20s 9ms/step - loss: 0.2387 - val_loss: 0.2871\n",
            "Epoch 208/400\n",
            "2188/2188 [==============================] - 20s 9ms/step - loss: 0.2342 - val_loss: 0.2896\n",
            "Epoch 209/400\n",
            "2188/2188 [==============================] - 20s 9ms/step - loss: 0.2374 - val_loss: 0.2550\n",
            "Epoch 210/400\n",
            "2188/2188 [==============================] - 20s 9ms/step - loss: 0.2353 - val_loss: 0.2509\n",
            "Epoch 211/400\n",
            "2188/2188 [==============================] - 20s 9ms/step - loss: 0.2431 - val_loss: 0.2605\n",
            "Epoch 212/400\n",
            "2188/2188 [==============================] - 20s 9ms/step - loss: 0.2385 - val_loss: 0.2742\n",
            "Epoch 213/400\n",
            "2188/2188 [==============================] - 20s 9ms/step - loss: 0.2367 - val_loss: 0.2711\n",
            "Epoch 214/400\n",
            "2188/2188 [==============================] - 20s 9ms/step - loss: 0.2391 - val_loss: 0.2774\n",
            "Epoch 215/400\n",
            "2188/2188 [==============================] - 20s 9ms/step - loss: 0.2433 - val_loss: 0.2515\n",
            "Epoch 216/400\n",
            "2188/2188 [==============================] - 20s 9ms/step - loss: 0.2370 - val_loss: 0.2567\n",
            "Epoch 217/400\n",
            "2188/2188 [==============================] - 20s 9ms/step - loss: 0.2384 - val_loss: 0.2862\n",
            "Epoch 218/400\n",
            "2188/2188 [==============================] - 20s 9ms/step - loss: 0.2388 - val_loss: 0.3118\n",
            "Epoch 219/400\n",
            "2188/2188 [==============================] - 20s 9ms/step - loss: 0.2377 - val_loss: 0.3052\n",
            "Epoch 220/400\n",
            "2188/2188 [==============================] - 20s 9ms/step - loss: 0.2361 - val_loss: 0.2764\n",
            "Epoch 221/400\n",
            "2188/2188 [==============================] - 20s 9ms/step - loss: 0.2381 - val_loss: 0.2867\n",
            "Epoch 222/400\n",
            "2188/2188 [==============================] - 20s 9ms/step - loss: 0.2363 - val_loss: 0.2544\n",
            "Epoch 223/400\n",
            "2188/2188 [==============================] - 20s 9ms/step - loss: 0.2363 - val_loss: 0.3111\n",
            "Epoch 224/400\n",
            "2188/2188 [==============================] - 20s 9ms/step - loss: 0.2396 - val_loss: 0.2501\n",
            "Epoch 225/400\n",
            "2188/2188 [==============================] - 20s 9ms/step - loss: 0.2352 - val_loss: 0.2541\n",
            "Epoch 226/400\n",
            "2188/2188 [==============================] - 20s 9ms/step - loss: 0.2381 - val_loss: 0.3710\n",
            "Epoch 227/400\n",
            "2188/2188 [==============================] - 20s 9ms/step - loss: 0.2360 - val_loss: 0.2802\n",
            "Epoch 228/400\n",
            "2188/2188 [==============================] - 20s 9ms/step - loss: 0.2406 - val_loss: 0.2641\n",
            "Epoch 229/400\n",
            "2188/2188 [==============================] - 20s 9ms/step - loss: 0.2367 - val_loss: 0.2451\n",
            "Epoch 230/400\n",
            "2188/2188 [==============================] - 20s 9ms/step - loss: 0.2379 - val_loss: 0.2942\n",
            "Epoch 231/400\n",
            "2188/2188 [==============================] - 20s 9ms/step - loss: 0.2387 - val_loss: 0.2681\n",
            "Epoch 232/400\n",
            "2188/2188 [==============================] - 20s 9ms/step - loss: 0.2365 - val_loss: 0.2575\n",
            "Epoch 233/400\n",
            "2188/2188 [==============================] - 20s 9ms/step - loss: 0.2382 - val_loss: 0.3161\n",
            "Epoch 234/400\n",
            "2188/2188 [==============================] - 20s 9ms/step - loss: 0.2365 - val_loss: 0.3845\n",
            "Epoch 235/400\n",
            "2188/2188 [==============================] - 20s 9ms/step - loss: 0.2404 - val_loss: 0.3314\n",
            "Epoch 236/400\n",
            "2188/2188 [==============================] - 20s 9ms/step - loss: 0.2363 - val_loss: 0.2751\n",
            "Epoch 237/400\n",
            "2188/2188 [==============================] - 20s 9ms/step - loss: 0.2362 - val_loss: 0.2488\n",
            "Epoch 238/400\n",
            "2188/2188 [==============================] - 20s 9ms/step - loss: 0.2373 - val_loss: 0.2595\n",
            "Epoch 239/400\n",
            "2188/2188 [==============================] - 20s 9ms/step - loss: 0.2379 - val_loss: 0.2563\n",
            "Epoch 240/400\n",
            "2188/2188 [==============================] - 20s 9ms/step - loss: 0.2357 - val_loss: 0.2502\n",
            "Epoch 241/400\n",
            "2188/2188 [==============================] - 20s 9ms/step - loss: 0.2341 - val_loss: 0.2530\n",
            "Epoch 242/400\n",
            "2188/2188 [==============================] - 20s 9ms/step - loss: 0.2334 - val_loss: 0.2637\n",
            "Epoch 243/400\n",
            "2188/2188 [==============================] - 20s 9ms/step - loss: 0.2369 - val_loss: 0.2580\n",
            "Epoch 244/400\n",
            "2188/2188 [==============================] - 20s 9ms/step - loss: 0.2330 - val_loss: 0.2903\n",
            "Epoch 245/400\n",
            "2188/2188 [==============================] - 20s 9ms/step - loss: 0.2350 - val_loss: 0.2759\n",
            "Epoch 246/400\n",
            "2188/2188 [==============================] - 20s 9ms/step - loss: 0.2369 - val_loss: 0.2801\n",
            "Epoch 247/400\n",
            "2188/2188 [==============================] - 20s 9ms/step - loss: 0.2342 - val_loss: 0.2423\n",
            "Epoch 248/400\n",
            "2188/2188 [==============================] - 20s 9ms/step - loss: 0.2346 - val_loss: 0.2499\n",
            "Epoch 249/400\n",
            "2188/2188 [==============================] - 20s 9ms/step - loss: 0.2332 - val_loss: 0.2521\n",
            "Epoch 250/400\n",
            "2188/2188 [==============================] - 20s 9ms/step - loss: 0.2354 - val_loss: 0.2427\n",
            "Epoch 251/400\n",
            "2188/2188 [==============================] - 20s 9ms/step - loss: 0.2345 - val_loss: 0.2694\n",
            "Epoch 252/400\n",
            "2188/2188 [==============================] - 20s 9ms/step - loss: 0.2358 - val_loss: 0.2681\n",
            "Epoch 253/400\n",
            "2188/2188 [==============================] - 20s 9ms/step - loss: 0.2352 - val_loss: 0.3480\n",
            "Epoch 254/400\n",
            "2188/2188 [==============================] - 20s 9ms/step - loss: 0.2361 - val_loss: 0.2440\n",
            "Epoch 255/400\n",
            "2188/2188 [==============================] - 20s 9ms/step - loss: 0.2338 - val_loss: 0.2570\n",
            "Epoch 256/400\n",
            "2188/2188 [==============================] - 20s 9ms/step - loss: 0.2346 - val_loss: 0.2679\n",
            "Epoch 257/400\n",
            "2188/2188 [==============================] - 20s 9ms/step - loss: 0.2345 - val_loss: 0.2912\n",
            "Epoch 258/400\n",
            "2188/2188 [==============================] - 20s 9ms/step - loss: 0.2356 - val_loss: 0.2599\n",
            "Epoch 259/400\n",
            "2188/2188 [==============================] - 20s 9ms/step - loss: 0.2317 - val_loss: 0.2610\n",
            "Epoch 260/400\n",
            "2188/2188 [==============================] - 20s 9ms/step - loss: 0.2340 - val_loss: 0.2440\n",
            "Epoch 261/400\n",
            "2188/2188 [==============================] - 20s 9ms/step - loss: 0.2328 - val_loss: 0.2989\n",
            "Epoch 262/400\n",
            "2188/2188 [==============================] - 20s 9ms/step - loss: 0.2310 - val_loss: 0.2761\n",
            "Epoch 263/400\n",
            "2188/2188 [==============================] - 20s 9ms/step - loss: 0.2332 - val_loss: 0.2426\n",
            "Epoch 264/400\n",
            "2188/2188 [==============================] - 20s 9ms/step - loss: 0.2326 - val_loss: 0.2377\n",
            "Epoch 265/400\n",
            "2188/2188 [==============================] - 20s 9ms/step - loss: 0.2349 - val_loss: 0.3050\n",
            "Epoch 266/400\n",
            "2188/2188 [==============================] - 20s 9ms/step - loss: 0.2322 - val_loss: 0.3035\n",
            "Epoch 267/400\n",
            "2188/2188 [==============================] - 20s 9ms/step - loss: 0.2351 - val_loss: 0.2835\n",
            "Epoch 268/400\n",
            "2188/2188 [==============================] - 20s 9ms/step - loss: 0.2340 - val_loss: 0.2645\n",
            "Epoch 269/400\n",
            "2188/2188 [==============================] - 20s 9ms/step - loss: 0.2351 - val_loss: 0.2707\n",
            "Epoch 270/400\n",
            "2188/2188 [==============================] - 20s 9ms/step - loss: 0.2331 - val_loss: 0.2482\n",
            "Epoch 271/400\n",
            "2188/2188 [==============================] - 20s 9ms/step - loss: 0.2294 - val_loss: 0.2684\n",
            "Epoch 272/400\n",
            "2188/2188 [==============================] - 20s 9ms/step - loss: 0.2317 - val_loss: 0.2375\n",
            "Epoch 273/400\n",
            "2188/2188 [==============================] - 20s 9ms/step - loss: 0.2284 - val_loss: 0.2625\n",
            "Epoch 274/400\n",
            "2188/2188 [==============================] - 20s 9ms/step - loss: 0.2296 - val_loss: 0.2652\n",
            "Epoch 275/400\n",
            "2188/2188 [==============================] - 20s 9ms/step - loss: 0.2335 - val_loss: 0.2876\n",
            "Epoch 276/400\n",
            "2188/2188 [==============================] - 20s 9ms/step - loss: 0.2318 - val_loss: 0.3043\n",
            "Epoch 277/400\n",
            "2188/2188 [==============================] - 20s 9ms/step - loss: 0.2307 - val_loss: 0.3265\n",
            "Epoch 278/400\n",
            "2188/2188 [==============================] - 20s 9ms/step - loss: 0.2312 - val_loss: 0.2316\n",
            "Epoch 279/400\n",
            "2188/2188 [==============================] - 20s 9ms/step - loss: 0.2344 - val_loss: 0.2509\n",
            "Epoch 280/400\n",
            "2188/2188 [==============================] - 20s 9ms/step - loss: 0.2339 - val_loss: 0.2531\n",
            "Epoch 281/400\n",
            "2188/2188 [==============================] - 20s 9ms/step - loss: 0.2286 - val_loss: 0.2687\n",
            "Epoch 282/400\n",
            "2188/2188 [==============================] - 20s 9ms/step - loss: 0.2344 - val_loss: 0.2733\n",
            "Epoch 283/400\n",
            "2188/2188 [==============================] - 20s 9ms/step - loss: 0.2333 - val_loss: 0.2407\n",
            "Epoch 284/400\n",
            "2188/2188 [==============================] - 20s 9ms/step - loss: 0.2286 - val_loss: 0.2900\n",
            "Epoch 285/400\n",
            "2188/2188 [==============================] - 20s 9ms/step - loss: 0.2298 - val_loss: 0.2681\n",
            "Epoch 286/400\n",
            "2188/2188 [==============================] - 20s 9ms/step - loss: 0.2298 - val_loss: 0.3038\n",
            "Epoch 287/400\n",
            "2188/2188 [==============================] - 20s 9ms/step - loss: 0.2326 - val_loss: 0.2844\n",
            "Epoch 288/400\n",
            "2188/2188 [==============================] - 20s 9ms/step - loss: 0.2296 - val_loss: 0.2589\n",
            "Epoch 289/400\n",
            "2188/2188 [==============================] - 20s 9ms/step - loss: 0.2276 - val_loss: 0.2624\n",
            "Epoch 290/400\n",
            "2188/2188 [==============================] - 20s 9ms/step - loss: 0.2310 - val_loss: 0.2490\n",
            "Epoch 291/400\n",
            "2188/2188 [==============================] - 20s 9ms/step - loss: 0.2287 - val_loss: 0.2448\n",
            "Epoch 292/400\n",
            "2188/2188 [==============================] - 20s 9ms/step - loss: 0.2323 - val_loss: 0.2650\n",
            "Epoch 293/400\n",
            "2188/2188 [==============================] - 20s 9ms/step - loss: 0.2309 - val_loss: 0.2519\n",
            "Epoch 294/400\n",
            "2188/2188 [==============================] - 20s 9ms/step - loss: 0.2289 - val_loss: 0.3499\n",
            "Epoch 295/400\n",
            "2188/2188 [==============================] - 20s 9ms/step - loss: 0.2287 - val_loss: 0.3184\n",
            "Epoch 296/400\n",
            "2188/2188 [==============================] - 20s 9ms/step - loss: 0.2310 - val_loss: 0.2589\n",
            "Epoch 297/400\n",
            "2188/2188 [==============================] - 20s 9ms/step - loss: 0.2315 - val_loss: 0.2474\n",
            "Epoch 298/400\n",
            "2188/2188 [==============================] - 20s 9ms/step - loss: 0.2299 - val_loss: 0.2324\n",
            "Epoch 299/400\n",
            "2188/2188 [==============================] - 20s 9ms/step - loss: 0.2273 - val_loss: 0.2463\n",
            "Epoch 300/400\n",
            "2188/2188 [==============================] - 20s 9ms/step - loss: 0.2254 - val_loss: 0.2539\n",
            "Epoch 301/400\n",
            "2188/2188 [==============================] - 20s 9ms/step - loss: 0.2338 - val_loss: 0.2340\n",
            "Epoch 302/400\n",
            "2188/2188 [==============================] - 20s 9ms/step - loss: 0.2291 - val_loss: 0.2439\n",
            "Epoch 303/400\n",
            "2188/2188 [==============================] - 20s 9ms/step - loss: 0.2309 - val_loss: 0.2354\n",
            "Epoch 304/400\n",
            "2188/2188 [==============================] - 20s 9ms/step - loss: 0.2263 - val_loss: 0.2667\n",
            "Epoch 305/400\n",
            "2188/2188 [==============================] - 20s 9ms/step - loss: 0.2290 - val_loss: 0.2396\n",
            "Epoch 306/400\n",
            "2188/2188 [==============================] - 20s 9ms/step - loss: 0.2303 - val_loss: 0.2431\n",
            "Epoch 307/400\n",
            "2188/2188 [==============================] - 20s 9ms/step - loss: 0.2302 - val_loss: 0.2653\n",
            "Epoch 308/400\n",
            "2188/2188 [==============================] - 20s 9ms/step - loss: 0.2255 - val_loss: 0.2323\n",
            "Epoch 309/400\n",
            "2188/2188 [==============================] - 20s 9ms/step - loss: 0.2267 - val_loss: 0.2924\n",
            "Epoch 310/400\n",
            "2188/2188 [==============================] - 20s 9ms/step - loss: 0.2278 - val_loss: 0.2720\n",
            "Epoch 311/400\n",
            "2188/2188 [==============================] - 20s 9ms/step - loss: 0.2253 - val_loss: 0.2943\n",
            "Epoch 312/400\n",
            "2188/2188 [==============================] - 20s 9ms/step - loss: 0.2272 - val_loss: 0.2486\n",
            "Epoch 313/400\n",
            "2188/2188 [==============================] - 20s 9ms/step - loss: 0.2320 - val_loss: 0.2627\n",
            "Epoch 314/400\n",
            "2188/2188 [==============================] - 20s 9ms/step - loss: 0.2294 - val_loss: 0.3085\n",
            "Epoch 315/400\n",
            "2188/2188 [==============================] - 20s 9ms/step - loss: 0.2305 - val_loss: 0.2948\n",
            "Epoch 316/400\n",
            "2188/2188 [==============================] - 20s 9ms/step - loss: 0.2291 - val_loss: 0.2426\n",
            "Epoch 317/400\n",
            "2188/2188 [==============================] - 20s 9ms/step - loss: 0.2273 - val_loss: 0.2538\n",
            "Epoch 318/400\n",
            "2188/2188 [==============================] - 20s 9ms/step - loss: 0.2253 - val_loss: 0.2770\n",
            "Epoch 319/400\n",
            "2188/2188 [==============================] - 20s 9ms/step - loss: 0.2275 - val_loss: 0.2999\n",
            "Epoch 320/400\n",
            "2188/2188 [==============================] - 20s 9ms/step - loss: 0.2292 - val_loss: 0.2504\n",
            "Epoch 321/400\n",
            "2188/2188 [==============================] - 20s 9ms/step - loss: 0.2272 - val_loss: 0.2540\n",
            "Epoch 322/400\n",
            "2188/2188 [==============================] - 20s 9ms/step - loss: 0.2256 - val_loss: 0.2349\n",
            "Epoch 323/400\n",
            "2188/2188 [==============================] - 20s 9ms/step - loss: 0.2322 - val_loss: 0.3499\n",
            "Epoch 324/400\n",
            "2188/2188 [==============================] - 20s 9ms/step - loss: 0.2261 - val_loss: 0.3131\n",
            "Epoch 325/400\n",
            "2188/2188 [==============================] - 20s 9ms/step - loss: 0.2283 - val_loss: 0.2766\n",
            "Epoch 326/400\n",
            "2188/2188 [==============================] - 20s 9ms/step - loss: 0.2269 - val_loss: 0.2888\n",
            "Epoch 327/400\n",
            "2188/2188 [==============================] - 20s 9ms/step - loss: 0.2307 - val_loss: 0.3022\n",
            "Epoch 328/400\n",
            "2188/2188 [==============================] - 20s 9ms/step - loss: 0.2281 - val_loss: 0.2794\n",
            "Epoch 329/400\n",
            "2188/2188 [==============================] - 20s 9ms/step - loss: 0.2225 - val_loss: 0.2750\n",
            "Epoch 330/400\n",
            "2188/2188 [==============================] - 20s 9ms/step - loss: 0.2271 - val_loss: 0.2488\n",
            "Epoch 331/400\n",
            "2188/2188 [==============================] - 20s 9ms/step - loss: 0.2258 - val_loss: 0.2531\n",
            "Epoch 332/400\n",
            "2188/2188 [==============================] - 20s 9ms/step - loss: 0.2285 - val_loss: 0.2648\n",
            "Epoch 333/400\n",
            "2188/2188 [==============================] - 20s 9ms/step - loss: 0.2279 - val_loss: 0.2472\n",
            "Epoch 334/400\n",
            "2188/2188 [==============================] - 20s 9ms/step - loss: 0.2253 - val_loss: 0.2470\n",
            "Epoch 335/400\n",
            "2188/2188 [==============================] - 20s 9ms/step - loss: 0.2244 - val_loss: 0.2325\n",
            "Epoch 336/400\n",
            "2188/2188 [==============================] - 20s 9ms/step - loss: 0.2240 - val_loss: 0.2678\n",
            "Epoch 337/400\n",
            "2188/2188 [==============================] - 20s 9ms/step - loss: 0.2290 - val_loss: 0.2543\n",
            "Epoch 338/400\n",
            "2188/2188 [==============================] - 20s 9ms/step - loss: 0.2259 - val_loss: 0.2514\n",
            "Epoch 339/400\n",
            "2188/2188 [==============================] - 20s 9ms/step - loss: 0.2257 - val_loss: 0.2410\n",
            "Epoch 340/400\n",
            "2188/2188 [==============================] - 20s 9ms/step - loss: 0.2301 - val_loss: 0.2560\n",
            "Epoch 341/400\n",
            "2188/2188 [==============================] - 20s 9ms/step - loss: 0.2253 - val_loss: 0.2476\n",
            "Epoch 342/400\n",
            "2188/2188 [==============================] - 20s 9ms/step - loss: 0.2247 - val_loss: 0.2715\n",
            "Epoch 343/400\n",
            "2188/2188 [==============================] - 20s 9ms/step - loss: 0.2284 - val_loss: 0.2479\n",
            "Epoch 344/400\n",
            "2188/2188 [==============================] - 20s 9ms/step - loss: 0.2292 - val_loss: 0.2353\n",
            "Epoch 345/400\n",
            "2188/2188 [==============================] - 20s 9ms/step - loss: 0.2227 - val_loss: 0.2874\n",
            "Epoch 346/400\n",
            "2188/2188 [==============================] - 20s 9ms/step - loss: 0.2271 - val_loss: 0.2737\n",
            "Epoch 347/400\n",
            "2188/2188 [==============================] - 20s 9ms/step - loss: 0.2281 - val_loss: 0.2711\n",
            "Epoch 348/400\n",
            "2188/2188 [==============================] - 20s 9ms/step - loss: 0.2264 - val_loss: 0.2375\n",
            "Epoch 349/400\n",
            "2188/2188 [==============================] - 20s 9ms/step - loss: 0.2230 - val_loss: 0.2486\n",
            "Epoch 350/400\n",
            "2188/2188 [==============================] - 20s 9ms/step - loss: 0.2240 - val_loss: 0.2442\n",
            "Epoch 351/400\n",
            "2188/2188 [==============================] - 20s 9ms/step - loss: 0.2258 - val_loss: 0.2972\n",
            "Epoch 352/400\n",
            "2188/2188 [==============================] - 20s 9ms/step - loss: 0.2226 - val_loss: 0.2527\n",
            "Epoch 353/400\n",
            "2188/2188 [==============================] - 20s 9ms/step - loss: 0.2259 - val_loss: 0.2431\n",
            "Epoch 354/400\n",
            "2188/2188 [==============================] - 20s 9ms/step - loss: 0.2279 - val_loss: 0.2924\n",
            "Epoch 355/400\n",
            "2188/2188 [==============================] - 20s 9ms/step - loss: 0.2251 - val_loss: 0.2409\n",
            "Epoch 356/400\n",
            "2188/2188 [==============================] - 20s 9ms/step - loss: 0.2228 - val_loss: 0.2618\n",
            "Epoch 357/400\n",
            "2188/2188 [==============================] - 20s 9ms/step - loss: 0.2255 - val_loss: 0.2448\n",
            "Epoch 358/400\n",
            "2188/2188 [==============================] - 20s 9ms/step - loss: 0.2247 - val_loss: 0.2563\n",
            "Epoch 359/400\n",
            "2188/2188 [==============================] - 20s 9ms/step - loss: 0.2241 - val_loss: 0.2367\n",
            "Epoch 360/400\n",
            "2188/2188 [==============================] - 20s 9ms/step - loss: 0.2242 - val_loss: 0.2783\n",
            "Epoch 361/400\n",
            "2188/2188 [==============================] - 20s 9ms/step - loss: 0.2279 - val_loss: 0.2811\n",
            "Epoch 362/400\n",
            "2188/2188 [==============================] - 20s 9ms/step - loss: 0.2246 - val_loss: 0.3066\n",
            "Epoch 363/400\n",
            "2188/2188 [==============================] - 20s 9ms/step - loss: 0.2240 - val_loss: 0.2670\n",
            "Epoch 364/400\n",
            "2188/2188 [==============================] - 20s 9ms/step - loss: 0.2265 - val_loss: 0.2492\n",
            "Epoch 365/400\n",
            "2188/2188 [==============================] - 20s 9ms/step - loss: 0.2230 - val_loss: 0.2960\n",
            "Epoch 366/400\n",
            "2188/2188 [==============================] - 20s 9ms/step - loss: 0.2261 - val_loss: 0.2552\n",
            "Epoch 367/400\n",
            "2188/2188 [==============================] - 20s 9ms/step - loss: 0.2223 - val_loss: 0.2485\n",
            "Epoch 368/400\n",
            "2188/2188 [==============================] - 20s 9ms/step - loss: 0.2245 - val_loss: 0.2608\n",
            "Epoch 369/400\n",
            "2188/2188 [==============================] - 20s 9ms/step - loss: 0.2226 - val_loss: 0.2398\n",
            "Epoch 370/400\n",
            "2188/2188 [==============================] - 20s 9ms/step - loss: 0.2228 - val_loss: 0.3064\n",
            "Epoch 371/400\n",
            "2188/2188 [==============================] - 20s 9ms/step - loss: 0.2231 - val_loss: 0.2509\n",
            "Epoch 372/400\n",
            "2188/2188 [==============================] - 20s 9ms/step - loss: 0.2227 - val_loss: 0.2818\n",
            "Epoch 373/400\n",
            "2188/2188 [==============================] - 20s 9ms/step - loss: 0.2253 - val_loss: 0.2557\n",
            "Epoch 374/400\n",
            "2188/2188 [==============================] - 20s 9ms/step - loss: 0.2232 - val_loss: 0.3115\n",
            "Epoch 375/400\n",
            "2188/2188 [==============================] - 20s 9ms/step - loss: 0.2232 - val_loss: 0.2642\n",
            "Epoch 376/400\n",
            "2188/2188 [==============================] - 20s 9ms/step - loss: 0.2247 - val_loss: 0.2675\n",
            "Epoch 377/400\n",
            "2188/2188 [==============================] - 20s 9ms/step - loss: 0.2263 - val_loss: 0.2455\n",
            "Epoch 378/400\n",
            "2188/2188 [==============================] - 20s 9ms/step - loss: 0.2226 - val_loss: 0.2748\n",
            "Epoch 379/400\n",
            "2188/2188 [==============================] - 20s 9ms/step - loss: 0.2237 - val_loss: 0.2426\n",
            "Epoch 380/400\n",
            "2188/2188 [==============================] - 20s 9ms/step - loss: 0.2245 - val_loss: 0.3305\n",
            "Epoch 381/400\n",
            "2188/2188 [==============================] - 20s 9ms/step - loss: 0.2219 - val_loss: 0.2574\n",
            "Epoch 382/400\n",
            "2188/2188 [==============================] - 20s 9ms/step - loss: 0.2258 - val_loss: 0.2601\n",
            "Epoch 383/400\n",
            "2188/2188 [==============================] - 20s 9ms/step - loss: 0.2246 - val_loss: 0.2834\n",
            "Epoch 384/400\n",
            "2188/2188 [==============================] - 20s 9ms/step - loss: 0.2230 - val_loss: 0.2698\n",
            "Epoch 385/400\n",
            "2188/2188 [==============================] - 20s 9ms/step - loss: 0.2259 - val_loss: 0.2442\n",
            "Epoch 386/400\n",
            "2188/2188 [==============================] - 20s 9ms/step - loss: 0.2259 - val_loss: 0.2547\n",
            "Epoch 387/400\n",
            "2188/2188 [==============================] - 20s 9ms/step - loss: 0.2244 - val_loss: 0.2924\n",
            "Epoch 388/400\n",
            "2188/2188 [==============================] - 20s 9ms/step - loss: 0.2234 - val_loss: 0.2707\n",
            "Epoch 389/400\n",
            "2188/2188 [==============================] - 20s 9ms/step - loss: 0.2217 - val_loss: 0.2980\n",
            "Epoch 390/400\n",
            "2188/2188 [==============================] - 20s 9ms/step - loss: 0.2236 - val_loss: 0.2364\n",
            "Epoch 391/400\n",
            "2188/2188 [==============================] - 20s 9ms/step - loss: 0.2240 - val_loss: 0.3484\n",
            "Epoch 392/400\n",
            "2188/2188 [==============================] - 20s 9ms/step - loss: 0.2256 - val_loss: 0.2799\n",
            "Epoch 393/400\n",
            "2188/2188 [==============================] - 20s 9ms/step - loss: 0.2226 - val_loss: 0.2606\n",
            "Epoch 394/400\n",
            "2188/2188 [==============================] - 20s 9ms/step - loss: 0.2220 - val_loss: 0.2279\n",
            "Epoch 395/400\n",
            "2188/2188 [==============================] - 20s 9ms/step - loss: 0.2242 - val_loss: 0.2305\n",
            "Epoch 396/400\n",
            "2188/2188 [==============================] - 20s 9ms/step - loss: 0.2198 - val_loss: 0.2553\n",
            "Epoch 397/400\n",
            "2188/2188 [==============================] - 20s 9ms/step - loss: 0.2216 - val_loss: 0.2586\n",
            "Epoch 398/400\n",
            "2188/2188 [==============================] - 20s 9ms/step - loss: 0.2208 - val_loss: 0.2621\n",
            "Epoch 399/400\n",
            "2188/2188 [==============================] - 20s 9ms/step - loss: 0.2240 - val_loss: 0.2545\n",
            "Epoch 400/400\n",
            "2188/2188 [==============================] - 21s 9ms/step - loss: 0.2224 - val_loss: 0.2620\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": [
              "download(\"download_99959996-0193-4054-9e45-128f9eb7b958\", \"VAE_Weights_try71.h5\", 1650368)"
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": [
              "download(\"download_a57fe1e3-9e93-4c64-8c36-fb21b5efd97b\", \"VAE_Model_try71.json\", 12045)"
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": [
              "download(\"download_15ce7c83-1f94-4652-83ef-75809904cefe\", \"Decoder_Model_try71.json\", 5033)"
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": [
              "download(\"download_409b6968-444f-451e-b87a-fe9046542798\", \"Decoder_Weights_try71.h5\", 737848)"
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gNIRlUFZ2zfT",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7d9b07bf-a824-4761-fcf2-52a556afd95b"
      },
      "source": [
        "# Load the trained model for testing purposes\r\n",
        "# Just when you are not going to train the model right now\r\n",
        "\r\n",
        "json_file = open('Decoder_Model.json', 'r')\r\n",
        "loaded_model_json = json_file.read()\r\n",
        "json_file.close()\r\n",
        "loaded_decoder= model_from_json(loaded_model_json)\r\n",
        "# load weights into new model\r\n",
        "loaded_decoder.load_weights(\"Decoder_Weights.h5\")\r\n",
        "print(\"Loaded model from disk\")\r\n",
        "\r\n",
        "loaded_decoder.compile(loss='mse', optimizer='adam')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Loaded model from disk\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4VxBrqF7T3lW"
      },
      "source": [
        "# Create a normal distribution and give it to the decoder as a generative network\r\n",
        "random_noise=np.random.random((2,256))\r\n",
        "prediction=loaded_decoder.predict(random_noise)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "atlZ9ZkoBImx"
      },
      "source": [
        "# Save the generated structures in a csv file.\r\n",
        "import csv\r\n",
        "counter = 0\r\n",
        "for i in range(prediction.shape[0]):\r\n",
        "    with open(f'generated_structure{counter}.csv','w') as f1:\r\n",
        "        writer=csv.writer(f1, delimiter='\\t')\r\n",
        "        num_rows = prediction[i].shape[0]\r\n",
        "        for j in range(num_rows):\r\n",
        "            row = prediction[i][j]\r\n",
        "            writer.writerow(row)\r\n",
        "    counter += 1"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}